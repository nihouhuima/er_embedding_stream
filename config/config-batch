# !mandatory! task to execute, operation available: "traning" (for embedding files), "match" (for ER task), "test" (for verify accuracy of matching) 
######### input config:
# task type: {train, test, match, train-test, train-match}
task:train
edgelists_file:pipeline/edgelists/dblp_acm-tableB.txt
# ######### for pre-training of stream
output_file:dblp_acm_B
# output path for operation "training" to embedding file
embeddings_file:pipeline/embeddings/dblp_acm_B.emb

experiment_type:ER
########## ground truth
# walks_file:/Users/rcap/Projects/embdi/pipeline/walks/amazon_google-ER.walks
# dataset_file is required for SM
# dataset_file:pipeline/datasets/amazon_google/amazon_google-master.csv

########### for batch test
# dataset_info:/home/zhongwei/Data_ingestion/Embdi/embdi/pipeline/info/info-amazon_google.txt
# #### ground truth
# match_file:/home/zhongwei/Data_ingestion/Embdi/EmbDI datasets/pipeline/matches/er-matches/matches-amazon_google.txt
# output_file:amazon_google-batch
# embeddings_file:pipeline/embeddings/amazon_google-batch.emb

########### walks config:
write_walks:true
sentence_length:60
n_sentences:default
follow_sub:false
smoothing_method:log
# Whether backtracking is allowed (whether it is possible to return to the previous node).
backtrack:true
# [If True, a similarity file will be needed to perfrom replacement]
repl_numbers:False
repl_strings:False
walks_strategy:basic
### {(str) prefix_of_nodes_to_flatten, all, ''} [The graph algorithm will split all nodes with a prefix listed here] [图算法将拆分所有带有此处所列前缀的节点］
flatten:tt

########### test config:
ntop:50
ncand:10
max_rank:3

########## Embeddings configuration:
learning_method:skipgram
window_size:3
n_dimensions:300
# choices for training algo : word2vec, fasttext, doc2vec
training_algorithm:word2vec


########## others:
intersection:false
mlflow:false


################################ kafka ###################################